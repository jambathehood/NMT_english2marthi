{"metadata":{"orig_nbformat":4,"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport string\nfrom string import digits\nimport re\nfrom sklearn.utils import shuffle","metadata":{"trusted":true},"execution_count":1,"outputs":[],"id":"43b18837-bd7e-4053-9075-7032bd33c18c"},{"cell_type":"code","source":"import tensorflow \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import LSTM, Input, Dense,Embedding, Concatenate, TimeDistributed\nfrom tensorflow.keras.models import Model,load_model, model_from_json\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import one_hot, Tokenizer\nfrom tensorflow.keras.callbacks import EarlyStopping","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","output_type":"stream"}],"id":"a9cbc471-5e17-4620-933e-414754a9610f"},{"cell_type":"code","source":"import pickle as pkl\nimport numpy as np","metadata":{"trusted":true},"execution_count":3,"outputs":[],"id":"13e675f8-418f-428f-95eb-4bb498380b0f"},{"cell_type":"code","source":"with open('hin.txt','r') as f:\n  data = f.read()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"03f429df-f2a4-4d34-aac5-b281692256fe"},{"cell_type":"code","source":"uncleaned_data_list = data.split('\\n')\nlen(uncleaned_data_list)\nuncleaned_data_list = uncleaned_data_list[:2900]\nlen(uncleaned_data_list)\nenglish_word = []\nhindi_word = []\ncleaned_data_list = []\nfor word in uncleaned_data_list:\n  english_word.append(word.split('\\t')[:-1][0])\n  hindi_word.append(word.split('\\t')[:-1][1])\nlanguage_data = pd.DataFrame(columns=['English','Hindi'])\nlanguage_data['English'] = english_word\nlanguage_data['Hindi'] = hindi_word\nlanguage_data.to_csv('language_data.csv', index=False)\n\nenglish_text = language_data['English'].values\nhindi_text = language_data['Hindi'].values\nlen(english_text), len(hindi_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"42607346-004d-4460-aabd-a82a19179124"},{"cell_type":"code","source":"#to lower case\nenglish_text_ = [x.lower() for x in english_text]\nhindi_text_ = [x.lower() for x in hindi_text]\n#removing inverted commas\nenglish_text_ = [re.sub(\"'\",'',x) for x in english_text_]\nhindi_text_ = [re.sub(\"'\",'',x) for x in hindi_text_]\ndef remove_punc(text_list):\n  table = str.maketrans('', '', string.punctuation)\n  removed_punc_text = []\n  for sent in text_list:\n    sentance = [w.translate(table) for w in sent.split(' ')]\n    removed_punc_text.append(' '.join(sentance))\n  return removed_punc_text\nenglish_text_ = remove_punc(english_text_)\nhindi_text_ = remove_punc(hindi_text_)\nremove_digits = str.maketrans('', '', digits)\nremoved_digits_text = []\nfor sent in english_text_:\n  sentance = [w.translate(remove_digits) for w in sent.split(' ')]\n  removed_digits_text.append(' '.join(sentance))\nenglish_text_ = removed_digits_text\n# removing the digits from the marathi sentances\nhindi_text_ = [re.sub(\"[१२३४५६७८९]\",\"\",x) for x in hindi_text_]\nhindi_text_ = [re.sub(\"[\\u200d]\",\"\",x) for x in hindi_text_]\n# removing the stating and ending whitespaces\nenglish_text_ = [x.strip() for x in english_text_]\nhindi_text_ = [x.strip() for x in hindi_text_]\n\n# Putting the start and end words in the marathi sentances\nhindi_text_ = [\"start \" + x + \" end\" for x in hindi_text_]\n# manipulated_marathi_text_\nhindi_text_[0], english_text_[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"8931d172-8a68-4eee-8a43-1a2df1df8031"},{"cell_type":"code","source":"# manipulated_marathi_text_\nhindi_text_[23], english_text_[23]","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"a23dc233-1fd6-46ee-9f1e-785c05857ab8"},{"cell_type":"code","source":"X = english_text_\nY = hindi_text_\nX_train, X_test, y_train, y_test=train_test_split(X,Y,test_size=0.1)\n\ndef Max_length(data):\n  max_length_ = max([len(x.split(' ')) for x in data])\n  return max_length_\n#Training data\nmax_length_english = Max_length(X_train)\nmax_length_hindi = Max_length(y_train)\n#Test data\nmax_length_english_test = Max_length(X_test)\nmax_length_hindi_test = Max_length(y_test)\nmax_length_hindi, max_length_english","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"24858971-988b-434d-b9b6-ae8803e80b78"},{"cell_type":"code","source":"englishTokenizer = Tokenizer()\nenglishTokenizer.fit_on_texts(X_train)\nEword2index = englishTokenizer.word_index\nvocab_size_source = len(Eword2index) + 1\nX_train = englishTokenizer.texts_to_sequences(X_train)\nX_train = pad_sequences(X_train, maxlen=max_length_english, padding='post')\nX_test = englishTokenizer.texts_to_sequences(X_test)\nX_test = pad_sequences(X_test, maxlen = max_length_english, padding='post')\nhindiTokenizer = Tokenizer()\nhindiTokenizer.fit_on_texts(y_train)\nHword2index = hindiTokenizer.word_index\nvocab_size_target = len(Hword2index) + 1\ny_train = hindiTokenizer.texts_to_sequences(y_train)\ny_train = pad_sequences(y_train, maxlen=max_length_hindi, padding='post')\ny_test = hindiTokenizer.texts_to_sequences(y_test)\ny_test = pad_sequences(y_test, maxlen = max_length_hindi, padding='post')\nvocab_size_source, vocab_size_target\n\nX_train[0], y_train[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"8426fac4-a2c4-43c7-b5be-99f811123079"},{"cell_type":"code","source":"with open('NMT_data.pkl','wb') as f:\n  pkl.dump([X_train, y_train, X_test, y_test],f)\nwith open('NMT_Etokenizer.pkl','wb') as f:\n  pkl.dump([vocab_size_source, Eword2index, englishTokenizer], f)\nwith open('NMT_Mtokenizer.pkl', 'wb') as f:\n  pkl.dump([vocab_size_target, Hword2index, hindiTokenizer], f)\nX_train = np.array(X_train)\ny_train = np.array(y_train)\nX_test = np.array(X_test)\ny_test = np.array(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"e02f145b"},{"cell_type":"code","source":"from attention import AttentionLayer\n","metadata":{"trusted":true},"execution_count":11,"outputs":[],"id":"cc4d39ac-8853-4043-ac06-f55bb56b01d4"},{"cell_type":"code","source":"from attention import AttentionLayer\ntensorflow.keras.backend.clear_session() \nlatent_dim = 500\n# Encoder \nencoder_inputs = Input(shape=(max_length_english,)) \nenc_emb = Embedding(vocab_size_source, latent_dim,trainable=True)(encoder_inputs)\n#LSTM 1 \nencoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \nencoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n#LSTM 2 \nencoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \nencoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n#LSTM 3 \nencoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \nencoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n# Set up the decoder. \ndecoder_inputs = Input(shape=(None,)) \ndec_emb_layer = Embedding(vocab_size_target, latent_dim,trainable=True) \ndec_emb = dec_emb_layer(decoder_inputs)\n#LSTM using encoder_states as initial state\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \ndecoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n#Attention Layer\nattn_layer = AttentionLayer(name='attention_layer') \nattn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n# Concat attention output and decoder LSTM output \ndecoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n#Dense layer\ndecoder_dense = TimeDistributed(Dense(vocab_size_target, activation='softmax')) \ndecoder_outputs = decoder_dense(decoder_concat_input)\n# Define the model\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs) \nplot_model(model, to_file='train_model.png', show_shapes=True)","metadata":{"trusted":true},"execution_count":12,"outputs":[],"id":"dbdaf04b"},{"cell_type":"code","source":"model.compile(optimizer='rmsprop',\n              loss='sparse_categorical_crossentropy', \n              metrics=['accuracy'])\n\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1)","metadata":{"trusted":true},"execution_count":13,"outputs":[],"id":"1245d21a"},{"cell_type":"code","source":"history = model.fit([X_train, y_train[:,:-1]], y_train.reshape(y_train.shape[0], y_train.shape[1],1)[:,1:], \n                    epochs=50, \n                    callbacks=[es],\n                    batch_size=512,\n                    validation_data = ([X_test, y_test[:,:-1]],           y_test.reshape(y_test.shape[0], y_test.shape[1], 1)[:,1:]))","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"b8b4e640"},{"cell_type":"code","source":"from matplotlib import pyplot \npyplot.plot(history.history['loss'], label='train') \npyplot.plot(history.history['val_loss'], label='test') \npyplot.legend() \npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"85c33e30"},{"cell_type":"code","source":"model_json = model.to_json()\nwith open(\"NMT_model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"NMT_model_weight.h5\")\nprint(\"Saved model to disk\")","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"ff0ea24d-f907-4ce1-9c7b-3d1ca1b6f2e1"},{"cell_type":"code","source":"# loading the model architecture and asigning the weights\njson_file = open('NMT_model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nmodel_loaded = model_from_json(loaded_model_json, custom_objects={'AttentionLayer': AttentionLayer})\n# load weights into new model\nmodel_loaded.load_weights(\"NMT_model_weight.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"a1cd83e5"},{"cell_type":"code","source":"latent_dim=500\n# encoder inference\nencoder_inputs = model_loaded.input[0]  #loading encoder_inputs\nencoder_outputs, state_h, state_c = model_loaded.layers[6].output #loading encoder_outputs\n#print(encoder_outputs.shape)\nencoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n# decoder inference\n# Below tensors will hold the states of the previous time step\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_hidden_state_input = Input(shape=(32,latent_dim))\n# Get the embeddings of the decoder sequence\ndecoder_inputs = model_loaded.layers[3].output\n#print(decoder_inputs.shape)\ndec_emb_layer = model_loaded.layers[5]\ndec_emb2= dec_emb_layer(decoder_inputs)\n# To predict the next word in the sequence, set the initial states to the states from the previous time step\ndecoder_lstm = model_loaded.layers[7]\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n#attention inference\nattn_layer = model_loaded.layers[8]\nattn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\nconcate = model_loaded.layers[9]\ndecoder_inf_concat = concate([decoder_outputs2, attn_out_inf])\n# A dense softmax layer to generate prob dist. over the target vocabulary\ndecoder_dense = model_loaded.layers[10]\ndecoder_outputs2 = decoder_dense(decoder_inf_concat)\n# Final decoder model\ndecoder_model = Model(\n[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n[decoder_outputs2] + [state_h2, state_c2])","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"83c57698"},{"cell_type":"code","source":"def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    e_out, e_h, e_c = encoder_model.predict(input_seq)\n# Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n# Chose the 'start' word as the first word of the target sequence\n    target_seq[0, 0] = Hword2index['start']\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n# Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        if sampled_token_index == 0:\n          break\n        else:\n          sampled_token = Hindex2word[sampled_token_index]\nif(sampled_token!='end'):\n              decoded_sentence += ' '+sampled_token\n# Exit condition: either hit max length or find stop word.\n              if (sampled_token == 'end' or len(decoded_sentence.split()) >= (26-1)):\n                  stop_condition = True\n# Update the target sequence (of length 1).\n              target_seq = np.zeros((1,1))\n              target_seq[0, 0] = sampled_token_index\n# Update internal states\n              e_h, e_c = h, c\nreturn decoded_sentence","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"adfabe0f"},{"cell_type":"code","source":"Eindex2word = englishTokenizer.index_word\nHindex2word = hindiTokenizer.index_word","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"425f8224"},{"cell_type":"code","source":"def seq2summary(input_seq):\n    newString=''\n    for i in input_seq:\n      if((i!=0 and i!=Hword2index['start']) and i!=Hword2index['end']):\n        newString=newString+Hindex2word[i]+' '\n    return newString\ndef seq2text(input_seq):\n    newString=''\n    for i in input_seq:\n      if(i!=0):\n        newString=newString+Eindex2word[i]+' '\n    return newString","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"7dc267a4"},{"cell_type":"code","source":"for i in range(10):  \n  print(\"Review:\",seq2text(X_test[i]))\n  print(\"Original summary:\",seq2summary(y_test[i]))\n  print(\"Predicted summary:\",decode_sequence(X_test[i].reshape(1,32)))\n  print(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"ce923ff9"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"15562041"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"ad33e686"},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport string\nfrom string import digits\nimport re\nfrom sklearn.utils import shuffle","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"c3667af1"},{"cell_type":"code","source":"import tensorflow \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import LSTM, Input, Dense,Embedding, Concatenate, TimeDistributed\nfrom tensorflow.keras.models import Model,load_model, model_from_json\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import one_hot, Tokenizer\nfrom tensorflow.keras.callbacks import EarlyStopping","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"8c1630af"},{"cell_type":"code","source":"import pickle as pkl\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"864d64cb"},{"cell_type":"code","source":"with open('hin.txt','r') as f:\n  data = f.read()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"03f429df-f2a4-4d34-aac5-b281692256fe"},{"cell_type":"code","source":"uncleaned_data_list = data.split('\\n')\nlen(uncleaned_data_list)\nuncleaned_data_list = uncleaned_data_list[:2900]\nlen(uncleaned_data_list)\nenglish_word = []\nhindi_word = []\ncleaned_data_list = []\nfor word in uncleaned_data_list:\n  english_word.append(word.split('\\t')[:-1][0])\n  hindi_word.append(word.split('\\t')[:-1][1])\nlanguage_data = pd.DataFrame(columns=['English','Hindi'])\nlanguage_data['English'] = english_word\nlanguage_data['Hindi'] = hindi_word\nlanguage_data.to_csv('language_data.csv', index=False)\n\nenglish_text = language_data['English'].values\nhindi_text = language_data['Hindi'].values\nlen(english_text), len(hindi_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"42607346-004d-4460-aabd-a82a19179124"},{"cell_type":"code","source":"#to lower case\nenglish_text_ = [x.lower() for x in english_text]\nhindi_text_ = [x.lower() for x in hindi_text]\n#removing inverted commas\nenglish_text_ = [re.sub(\"'\",'',x) for x in english_text_]\nhindi_text_ = [re.sub(\"'\",'',x) for x in hindi_text_]\ndef remove_punc(text_list):\n  table = str.maketrans('', '', string.punctuation)\n  removed_punc_text = []\n  for sent in text_list:\n    sentance = [w.translate(table) for w in sent.split(' ')]\n    removed_punc_text.append(' '.join(sentance))\n  return removed_punc_text\nenglish_text_ = remove_punc(english_text_)\nhindi_text_ = remove_punc(hindi_text_)\nremove_digits = str.maketrans('', '', digits)\nremoved_digits_text = []\nfor sent in english_text_:\n  sentance = [w.translate(remove_digits) for w in sent.split(' ')]\n  removed_digits_text.append(' '.join(sentance))\nenglish_text_ = removed_digits_text\n# removing the digits from the marathi sentances\nhindi_text_ = [re.sub(\"[१२३४५६७८९]\",\"\",x) for x in hindi_text_]\nhindi_text_ = [re.sub(\"[\\u200d]\",\"\",x) for x in hindi_text_]\n# removing the stating and ending whitespaces\nenglish_text_ = [x.strip() for x in english_text_]\nhindi_text_ = [x.strip() for x in hindi_text_]\n\n# Putting the start and end words in the marathi sentances\nhindi_text_ = [\"start \" + x + \" end\" for x in hindi_text_]\n# manipulated_marathi_text_\nhindi_text_[0], english_text_[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"8931d172-8a68-4eee-8a43-1a2df1df8031"},{"cell_type":"code","source":"# manipulated_marathi_text_\nhindi_text_[23], english_text_[23]","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"a23dc233-1fd6-46ee-9f1e-785c05857ab8"},{"cell_type":"code","source":"X = english_text_\nY = hindi_text_\nX_train, X_test, y_train, y_test=train_test_split(X,Y,test_size=0.1)\n\ndef Max_length(data):\n  max_length_ = max([len(x.split(' ')) for x in data])\n  return max_length_\n#Training data\nmax_length_english = Max_length(X_train)\nmax_length_hindi = Max_length(y_train)\n#Test data\nmax_length_english_test = Max_length(X_test)\nmax_length_hindi_test = Max_length(y_test)\nmax_length_hindi, max_length_english","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"24858971-988b-434d-b9b6-ae8803e80b78"},{"cell_type":"code","source":"englishTokenizer = Tokenizer()\nenglishTokenizer.fit_on_texts(X_train)\nEword2index = englishTokenizer.word_index\nvocab_size_source = len(Eword2index) + 1\nX_train = englishTokenizer.texts_to_sequences(X_train)\nX_train = pad_sequences(X_train, maxlen=max_length_english, padding='post')\nX_test = englishTokenizer.texts_to_sequences(X_test)\nX_test = pad_sequences(X_test, maxlen = max_length_english, padding='post')\nhindiTokenizer = Tokenizer()\nhindiTokenizer.fit_on_texts(y_train)\nHword2index = hindiTokenizer.word_index\nvocab_size_target = len(Hword2index) + 1\ny_train = hindiTokenizer.texts_to_sequences(y_train)\ny_train = pad_sequences(y_train, maxlen=max_length_hindi, padding='post')\ny_test = hindiTokenizer.texts_to_sequences(y_test)\ny_test = pad_sequences(y_test, maxlen = max_length_hindi, padding='post')\nvocab_size_source, vocab_size_target\n\nX_train[0], y_train[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"8426fac4-a2c4-43c7-b5be-99f811123079"},{"cell_type":"code","source":"with open('NMT_data.pkl','wb') as f:\n  pkl.dump([X_train, y_train, X_test, y_test],f)\nwith open('NMT_Etokenizer.pkl','wb') as f:\n  pkl.dump([vocab_size_source, Eword2index, englishTokenizer], f)\nwith open('NMT_Mtokenizer.pkl', 'wb') as f:\n  pkl.dump([vocab_size_target, Hword2index, hindiTokenizer], f)\nX_train = np.array(X_train)\ny_train = np.array(y_train)\nX_test = np.array(X_test)\ny_test = np.array(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"e02f145b"},{"cell_type":"code","source":"from attention import AttentionLayer\ntensorflow.keras.backend.clear_session() \nlatent_dim = 500\n# Encoder \nencoder_inputs = Input(shape=(max_length_english,)) \nenc_emb = Embedding(vocab_size_source, latent_dim,trainable=True)(encoder_inputs)\n#LSTM 1 \nencoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \nencoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n#LSTM 2 \nencoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \nencoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n#LSTM 3 \nencoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \nencoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n# Set up the decoder. \ndecoder_inputs = Input(shape=(None,)) \ndec_emb_layer = Embedding(vocab_size_target, latent_dim,trainable=True) \ndec_emb = dec_emb_layer(decoder_inputs)\n#LSTM using encoder_states as initial state\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \ndecoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n#Attention Layer\nattn_layer = AttentionLayer(name='attention_layer') \nattn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n# Concat attention output and decoder LSTM output \ndecoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n#Dense layer\ndecoder_dense = TimeDistributed(Dense(vocab_size_target, activation='softmax')) \ndecoder_outputs = decoder_dense(decoder_concat_input)\n# Define the model\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs) \nplot_model(model, to_file='train_model.png', show_shapes=True)","metadata":{"trusted":true},"execution_count":12,"outputs":[],"id":"dbdaf04b"},{"cell_type":"code","source":"model.compile(optimizer='rmsprop',\n              loss='sparse_categorical_crossentropy', \n              metrics=['accuracy'])\n\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1)","metadata":{"trusted":true},"execution_count":13,"outputs":[],"id":"1245d21a"},{"cell_type":"code","source":"history = model.fit([X_train, y_train[:,:-1]], y_train.reshape(y_train.shape[0], y_train.shape[1],1)[:,1:], \n                    epochs=50, \n                    callbacks=[es],\n                    batch_size=512,\n                    validation_data = ([X_test, y_test[:,:-1]],           y_test.reshape(y_test.shape[0], y_test.shape[1], 1)[:,1:]))","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"b8b4e640"},{"cell_type":"code","source":"from matplotlib import pyplot \npyplot.plot(history.history['loss'], label='train') \npyplot.plot(history.history['val_loss'], label='test') \npyplot.legend() \npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"85c33e30"},{"cell_type":"code","source":"model_json = model.to_json()\nwith open(\"NMT_model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"NMT_model_weight.h5\")\nprint(\"Saved model to disk\")","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"ff0ea24d-f907-4ce1-9c7b-3d1ca1b6f2e1"},{"cell_type":"code","source":"# loading the model architecture and asigning the weights\njson_file = open('NMT_model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nmodel_loaded = model_from_json(loaded_model_json, custom_objects={'AttentionLayer': AttentionLayer})\n# load weights into new model\nmodel_loaded.load_weights(\"NMT_model_weight.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"a1cd83e5"},{"cell_type":"code","source":"latent_dim=500\n# encoder inference\nencoder_inputs = model_loaded.input[0]  #loading encoder_inputs\nencoder_outputs, state_h, state_c = model_loaded.layers[6].output #loading encoder_outputs\n#print(encoder_outputs.shape)\nencoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n# decoder inference\n# Below tensors will hold the states of the previous time step\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_hidden_state_input = Input(shape=(32,latent_dim))\n# Get the embeddings of the decoder sequence\ndecoder_inputs = model_loaded.layers[3].output\n#print(decoder_inputs.shape)\ndec_emb_layer = model_loaded.layers[5]\ndec_emb2= dec_emb_layer(decoder_inputs)\n# To predict the next word in the sequence, set the initial states to the states from the previous time step\ndecoder_lstm = model_loaded.layers[7]\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n#attention inference\nattn_layer = model_loaded.layers[8]\nattn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\nconcate = model_loaded.layers[9]\ndecoder_inf_concat = concate([decoder_outputs2, attn_out_inf])\n# A dense softmax layer to generate prob dist. over the target vocabulary\ndecoder_dense = model_loaded.layers[10]\ndecoder_outputs2 = decoder_dense(decoder_inf_concat)\n# Final decoder model\ndecoder_model = Model(\n[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n[decoder_outputs2] + [state_h2, state_c2])","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"83c57698"},{"cell_type":"code","source":"def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    e_out, e_h, e_c = encoder_model.predict(input_seq)\n# Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n# Chose the 'start' word as the first word of the target sequence\n    target_seq[0, 0] = Hword2index['start']\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n# Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        if sampled_token_index == 0:\n          break\n        else:\n          sampled_token = Hindex2word[sampled_token_index]\nif(sampled_token!='end'):\n              decoded_sentence += ' '+sampled_token\n# Exit condition: either hit max length or find stop word.\n              if (sampled_token == 'end' or len(decoded_sentence.split()) >= (26-1)):\n                  stop_condition = True\n# Update the target sequence (of length 1).\n              target_seq = np.zeros((1,1))\n              target_seq[0, 0] = sampled_token_index\n# Update internal states\n              e_h, e_c = h, c\nreturn decoded_sentence","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"adfabe0f"},{"cell_type":"code","source":"Eindex2word = englishTokenizer.index_word\nHindex2word = hindiTokenizer.index_word","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"425f8224"},{"cell_type":"code","source":"def seq2summary(input_seq):\n    newString=''\n    for i in input_seq:\n      if((i!=0 and i!=Hword2index['start']) and i!=Hword2index['end']):\n        newString=newString+Hindex2word[i]+' '\n    return newString\ndef seq2text(input_seq):\n    newString=''\n    for i in input_seq:\n      if(i!=0):\n        newString=newString+Eindex2word[i]+' '\n    return newString","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"7dc267a4"},{"cell_type":"code","source":"for i in range(10):  \n  print(\"Review:\",seq2text(X_test[i]))\n  print(\"Original summary:\",seq2summary(y_test[i]))\n  print(\"Predicted summary:\",decode_sequence(X_test[i].reshape(1,32)))\n  print(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"ce923ff9"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"15562041"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"ad33e686"}]}